{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "167zDReqn7HN"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume our classes are $y\\in \\{0,1\\}$. We model our data as $$P(y_i=1|x_i;\\theta)\\approx \\frac{1}{1+\\exp(-\\theta^T x)}.$$\n",
        "We try to choose $\\theta$ such that the likelihood $$\\prod P(y_i|x_i;\\theta)$$ is maximal. This can't be analytically solved, so we use gradient descent.\n",
        "\n",
        "We apply log, multiply by $-1$, and try to minize using gradient descent (we analytically can calculate the partial derivatives, see attached pdf for more details on the math)."
      ],
      "metadata": {
        "id": "8hkDJda3PxDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x,coefs):\n",
        "  return 1/(1+np.exp(-x@coefs))\n",
        "\n",
        "def predict(X,theta):\n",
        "  return np.array(sigmoid(X,theta)>0.5,int)\n",
        "\n",
        "def gradient(theta,X,Y):\n",
        "  difference=predict(X,theta)-Y\n",
        "  # all the MATH is hidden in this expression, the rest is routine code\n",
        "  # see pdf for more details\n",
        "  return X.transpose()@difference/Y.shape[0]\n",
        "\n",
        "def GradientDescent(X,Y,iterations,learningRate):\n",
        "  theta=np.zeros(X.shape[1])\n",
        "  for iter in range(iterations):\n",
        "    theta=theta-learningRate*(gradient(theta,X,Y))\n",
        "  return theta"
      ],
      "metadata": {
        "id": "-K6lRqcd-leK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply this to some toy dataset"
      ],
      "metadata": {
        "id": "Ivm4WJe0RDep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X,Y = datasets.load_breast_cancer(return_X_y=True)\n",
        "\n",
        "#adds a column of ones (to account for bias)\n",
        "X=np.c_[X,np.ones(X.shape[0])]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "hIbulaWX1RCN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta=GradientDescent(X_train,y_train, 200,0.1)\n",
        "prediction=predict(X_test,theta)"
      ],
      "metadata": {
        "id": "ha9oxYV8ABd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49dcf96b-1e4c-46bb-9cd1-c267117806a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-ef7f609fb1b5>:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-x@coefs))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(prediction==y_test)/y_test.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aUDD5EXI8xt",
        "outputId": "79005bfd-f64e-4536-d5d8-3954047c3b77"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8947368421052632"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hooray! Accuracy of 0.89 -> our algorithm works.\n",
        "\n",
        "Be careful: We do not get back the same coefficients we started with, but the ratios are preserved, and that is enough"
      ],
      "metadata": {
        "id": "X5BUPdCjMoqs"
      }
    }
  ]
}