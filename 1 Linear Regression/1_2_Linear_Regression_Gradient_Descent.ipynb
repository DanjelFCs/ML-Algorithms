{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We continue using notation from section 1.1.\n",
        "The partial derivatives are\n",
        "$$\\frac{\\partial I}{\\partial w_k}(w_1, \\cdots ,w_D)=\\frac{1}{N}\\sum_{j=1}^N x^k_j\\biggl( \\sum_{i=1}^{D} w_i x_j^i- t_j\\biggr).$$\n",
        "\n",
        "We apply **gradient descent** using the following update rule\n",
        "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial I}{\\partial \\mathbf{w}}$$\n",
        "where $\\alpha$ is the learning rate.\n",
        "\n",
        "Gradient descent is guaranteed to converge, since our loss function is **CONVEX**!"
      ],
      "metadata": {
        "id": "0EDdX6z8auuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# We create linear model data - 100 data points, 4 features\n",
        "N=100\n",
        "D=4\n",
        "# these will be the coefficients (a0 will be constant term)\n",
        "a0,a1,a2,a3=1,2,0.5,3\n",
        "c=[a0,a1,a2,a3]\n",
        "\n",
        "# random data\n",
        "X=np.random.rand(100,4)\n",
        "#bias\n",
        "X[:,0]=np.ones(100)\n",
        "Y=a1*X[:,1]+a2*X[:,2]+a3*X[:,3]+a0*X[:,0]+0.3*np.random.rand(100) #this last term is some random noise/error"
      ],
      "metadata": {
        "id": "3_uC_9lXa4Yg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We just implement the above formula (this is all the math, no more)\n",
        "def CalculateGradient(w):\n",
        "  g=np.zeros(4)\n",
        "  for k in range(D):\n",
        "    for j in range(N):\n",
        "      s=0\n",
        "      for i in range(D):\n",
        "        s += w[i]*X[j][i]\n",
        "      s-=Y[j]\n",
        "      s*=X[j][k]\n",
        "      g[k]+=s\n",
        "  g=1/N*g\n",
        "  return g"
      ],
      "metadata": {
        "id": "btFETja3fUXn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is just iterating the process\n",
        "def GradientDescent(startingCoefs, alpha, iterations):\n",
        "  w=startingCoefs\n",
        "  for i in range(iterations):\n",
        "    w=w-alpha*CalculateGradient(w)\n",
        "  return w"
      ],
      "metadata": {
        "id": "keEVFJNVg4uN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GradientDescent(np.zeros(4), 0.2, 500)"
      ],
      "metadata": {
        "id": "PUPiHWQ4la9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf3b0a06-67df-43a6-8d1d-3bfbd26891f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.15024692, 2.03398601, 0.50156973, 2.96166711])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our strategy works just fine, but is not as good as the direct solution, but is a lot more fun.\n",
        "\n",
        "We do need quite a bit of iterations... A lot more computing than by the direct way of computing. But for pedagogical purposes it is just fine. This algo will be used whenever we can't analytically solve for the optimal parameters as in this case (see logistic regression)."
      ],
      "metadata": {
        "id": "WBXysLL8sPV8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UjGRMg4Os_gW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}